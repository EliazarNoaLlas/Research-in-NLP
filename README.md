# üöÄ Space Biology Knowledge Graph Pipeline

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![NASA](https://img.shields.io/badge/NASA-Space%20Biology-red.svg)](https://www.nasa.gov/biological-physical)
[![Status](https://img.shields.io/badge/Status-Active%20Development-yellow.svg)]()

> **Pipeline inteligente de ingesta y procesamiento de datos cient√≠ficos para construir un knowledge graph de biolog√≠a espacial basado en publicaciones de NASA y fuentes asociadas.**

## üìã Tabla de Contenidos

- [üéØ Descripci√≥n del Proyecto](#-descripci√≥n-del-proyecto)
- [‚ú® Caracter√≠sticas Principales](#-caracter√≠sticas-principales)
- [üèóÔ∏è Arquitectura](#Ô∏è-arquitectura)
- [üîß Instalaci√≥n](#-instalaci√≥n)
- [üöÄ Uso R√°pido](#-uso-r√°pido)
- [üìä Fuentes de Datos](#-fuentes-de-datos)
- [ü§ñ Procesamiento NLP](#-procesamiento-nlp)
- [üîÑ Pipeline ETL](#-pipeline-etl)
- [üìà M√©tricas y Calidad](#-m√©tricas-y-calidad)
- [üóÇÔ∏è Formatos de Salida](#Ô∏è-formatos-de-salida)
- [‚öôÔ∏è Configuraci√≥n Avanzada](#Ô∏è-configuraci√≥n-avanzada)
- [üß™ Testing y Validaci√≥n](#-testing-y-validaci√≥n)
- [üìù Logging y Monitoreo](#-logging-y-monitoreo)
- [ü§ù Contribuci√≥n](#-contribuci√≥n)
- [üìÑ Licencia](#-licencia)

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa un **pipeline ETL inteligente** dise√±ado para el desaf√≠o NASA Space Biology Knowledge Graph. El sistema procesa autom√°ticamente publicaciones cient√≠ficas de biolog√≠a espacial, extrae informaci√≥n sem√°ntica relevante y la convierte a formatos est√°ndar para construir un knowledge graph comprensivo.

### Problema que Resuelve

- **Fragmentaci√≥n del conocimiento**: La informaci√≥n sobre biolog√≠a espacial est√° dispersa en m√∫ltiples repositorios
- **Falta de estandarizaci√≥n**: Diferentes formatos y metadatos entre fuentes
- **Complejidad de procesamiento**: Necesidad de t√©cnicas avanzadas de NLP biom√©dico
- **Escalabilidad**: Procesamiento eficiente de miles de art√≠culos cient√≠ficos

### Soluci√≥n Implementada

Pipeline automatizado que:
1. **Ingesta** datos desde m√∫ltiples fuentes NASA de forma inteligente
2. **Extrae** informaci√≥n usando web scraping profesional y APIs
3. **Procesa** contenido con NLP biom√©dico especializado (ScispaCy, BioBERT)
4. **Transforma** a formatos sem√°nticos est√°ndar (JSON-LD, RDF)
5. **Valida** calidad y detecta duplicados autom√°ticamente

## ‚ú® Caracter√≠sticas Principales

### üîç Extracci√≥n Inteligente
- **Web scraping profesional** con manejo de rate limiting
- **M√∫ltiples estrategias de extracci√≥n** (metadatos, selectores CSS, fallbacks)
- **Integraci√≥n con APIs** de PubMed/PMC para enriquecimiento
- **Detecci√≥n autom√°tica** de PMIDs, PMCIDs, DOIs

### üß† Procesamiento NLP Avanzado
- **ScispaCy** para entidades biom√©dicas
- **BioBERT embeddings** para representaci√≥n sem√°ntica
- **Extracci√≥n especializada** de entidades de biolog√≠a espacial
- **Detecci√≥n de relaciones** entre conceptos

### üîÑ Pipeline Robusto
- **Arquitectura as√≠ncrona** para alta concurrencia
- **Manejo inteligente de errores** con reintentos exponenciales
- **Validaci√≥n multinivel** de calidad de datos
- **Detecci√≥n de duplicados** usando fuzzy matching

### üìä Calidad de Datos
- **Scoring autom√°tico** de calidad de art√≠culos
- **Validaciones sem√°nticas** usando ontolog√≠as
- **Normalizaci√≥n autom√°tica** de metadatos
- **Reportes detallados** de estad√≠sticas

## üèóÔ∏è Arquitectura

```mermaid
graph TD
    A[CSV Input] --> B[DataIngestionPipeline]
    B --> C[Web Scraping Engine]
    C --> D[Metadata Extractor]
    D --> E[Quality Validator]
    E --> F[Duplicate Detector]
    F --> G[BiologyNLPProcessor]
    G --> H[Entity Extractor]
    H --> I[Relationship Detector]
    I --> J[Format Converter]
    J --> K[JSON-LD Output]
    J --> L[RDF Triples]
    J --> M[Knowledge Graph Entities]
```

### Componentes Principales

| Componente | Responsabilidad | Tecnolog√≠as |
|------------|-----------------|-------------|
| **DataIngestionPipeline** | Orquestaci√≥n del pipeline ETL | asyncio, aiohttp |
| **Web Scraping Engine** | Extracci√≥n robusta de contenido | BeautifulSoup4, requests |
| **BiologyNLPProcessor** | Procesamiento de lenguaje natural | ScispaCy, BioBERT, transformers |
| **Format Converter** | Transformaci√≥n a formatos sem√°nticos | rdflib, JSON-LD |
| **Quality Validator** | Validaci√≥n y scoring de calidad | Custom ML scoring |
| **Duplicate Detector** | Detecci√≥n de contenido duplicado | FuzzyWuzzy, hashing |

## üîß Instalaci√≥n

### Requisitos del Sistema
- Python 3.8+
- 8GB RAM m√≠nimo (recomendado 16GB)
- 5GB espacio en disco
- Conexi√≥n a Internet estable

### Instalaci√≥n R√°pida

```bash
# Clonar repositorio
git clone https://github.com/FiboNacci-codder/Research-in-NLP
cd space-biology-kg-pipeline

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Instalar dependencias b√°sicas
pip install -r requirements.txt

# Instalar modelo ScispaCy (1.2GB)
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz

# Descargar recursos NLTK
python -c "import nltk; nltk.download('stopwords')"
```

### Instalaci√≥n con Docker

```bash
# Construir imagen
docker build -t space-biology-pipeline .

# Ejecutar contenedor
docker run -v $(pwd)/output:/app/output space-biology-pipeline
```

### requirements.txt
```txt
aiohttp>=3.8.0
pandas>=1.5.0
spacy>=3.4.0
beautifulsoup4>=4.11.0
transformers>=4.21.0
torch>=1.12.0
rdflib>=6.2.0
fuzzywuzzy[speedup]>=0.18.0
nltk>=3.7
requests-cache>=0.9.0
tenacity>=8.0.0
pytest>=7.0.0
pytest-asyncio>=0.19.0
```

## üöÄ Uso R√°pido

### Ejecuci√≥n B√°sica

```bash
# Ejecutar pipeline completo
python space_biology_etl.py

# Con archivo CSV personalizado
python space_biology_etl.py --csv-path /path/to/custom.csv

# Procesar solo primeros N art√≠culos (testing)
python space_biology_etl.py --limit 10

# Con configuraci√≥n custom
python space_biology_etl.py --config config.yaml
```

### Uso Program√°tico

```python
import asyncio
from space_biology_etl import DataIngestionPipeline, BiologyNLPProcessor

async def main():
    async with DataIngestionPipeline() as pipeline:
        # Cargar datos
        articles_data = await pipeline.load_sb_publications_csv()
        
        # Extraer art√≠culos
        articles = await pipeline.extract_papers(articles_data[:5])
        
        # Procesar con NLP
        nlp_processor = BiologyNLPProcessor()
        processed = await nlp_processor.process_articles(articles)
        
        # Convertir a formatos sem√°nticos
        standardized = pipeline.standardize_format(processed)
        
        print(f"Procesados {len(standardized)} art√≠culos")

# Ejecutar
asyncio.run(main())
```

### Configuraci√≥n R√°pida

```python
# Ajustar concurrencia
pipeline.semaphore_limit = 10  # M√°ximo 10 requests concurrentes

# Cambiar umbral de calidad
pipeline.quality_threshold = 0.7  # Solo art√≠culos con >70% calidad

# Modificar rate limiting
pipeline.request_delay = 1.0  # 1 segundo entre requests
```

## üìä Fuentes de Datos

### Fuentes Primarias

| Fuente | Descripci√≥n | Cobertura | API |
|--------|-------------|-----------|-----|
| **SB Publications** | 608 publicaciones curadas de biolog√≠a espacial | 2010-2024 | ‚ùå |
| **NASA OSDR** | Open Science Data Repository | 500+ experimentos | ‚úÖ |
| **GeneLab** | Datos √≥micos multimodales | Gen√≥mica/Transcript√≥mica | ‚úÖ |
| **NSLSL** | NASA Space Life Sciences Library | Literatura global | ‚ùå |
| **NASA Task Book** | Proyectos de investigaci√≥n financiados | BPS/HRP projects | ‚ùå |
| **PubMed/PMC** | Enriquecimiento de metadatos | Biom√©dica global | ‚úÖ |

### Formato de Entrada (SB Publications CSV)

```csv
Title,Link
"Mice in Bion-M 1 space mission: training and selection",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4136787/
"Effects of microgravity on bone density",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5847291/
```

### Metadatos Extra√≠dos

- **Bibliogr√°ficos**: T√≠tulo, autores, afiliaciones, journal, fecha
- **Identificadores**: DOI, PMID, PMCID
- **Contenido**: Abstract, texto completo, keywords, t√©rminos MeSH
- **Sem√°nticos**: Entidades biom√©dicas, relaciones, embeddings

## ü§ñ Procesamiento NLP

### Modelos Utilizados

#### ScispaCy (en_core_sci_lg)
```python
# Entidades biom√©dicas detectadas
SPECIES, TAXON      # Organismos
GENE, PROTEIN       # Informaci√≥n gen√©tica
CHEMICAL, DRUG      # Compuestos qu√≠micos
DISEASE, CONDITION  # Condiciones m√©dicas
```

#### BioBERT (dmis-lab/biobert-v1.1)
```python
# Embeddings sem√°nticos de 768 dimensiones
embeddings = biobert_model.encode("microgravity effects on bone density")
# [0.1234, -0.5678, 0.9012, ...]
```

### Entidades Espec√≠ficas de Biolog√≠a Espacial

#### Organismos Modelo
```python
organisms = {
    'human', 'mouse', 'mice', 'rat', 'drosophila', 
    'arabidopsis', 'c. elegans', 'e. coli', 'bacillus subtilis'
}
```

#### Condiciones Espaciales
```python
space_conditions = {
    'microgravity', 'hypergravity', 'weightlessness', 
    'cosmic radiation', 'space radiation', 'magnetic field',
    'parabolic flight', 'centrifuge', 'clinostat'
}
```

#### Sistemas Biol√≥gicos
```python
biological_systems = {
    'cardiovascular', 'musculoskeletal', 'nervous system',
    'bone density', 'muscle atrophy', 'osteoporosis',
    'immune system', 'heart', 'brain', 'kidney'
}
```

#### T√©cnicas Experimentales
```python
experiments = {
    'gene expression', 'protein analysis', 'cell culture',
    'tissue engineering', 'metabolomics', 'proteomics',
    'transcriptomics', 'histology', 'microscopy'
}
```

### Patrones de Extracci√≥n

```python
entity_patterns = {
    'genes': r'\b[A-Z][A-Z0-9]{2,}(?:-[A-Z0-9]+)?\b',
    'proteins': r'\b[A-Z][a-z]+(?:-\d+)?(?:\s+[A-Z][a-z]*)*\b',
    'measurements': r'\d+(?:\.\d+)?\s*(?:mg|g|kg|ml|l|Œºm|mm|cm|¬∞C|%)',
    'time_periods': r'\d+\s*(?:days?|weeks?|months?|years?|hrs?)'
}
```

## üîÑ Pipeline ETL

### Fase Extract (Extracci√≥n)

```python
async def extract_papers(self, articles_data: List[Dict]) -> List[Article]:
    """
    1. Carga as√≠ncrona de URLs
    2. Web scraping inteligente con m√∫ltiples selectores
    3. Extracci√≥n de metadatos estructurados
    4. Enriquecimiento con APIs externas
    5. Rate limiting y manejo de errores
    """
```

**Estrategias de Extracci√≥n:**
- **Metadatos Structured Data**: JSON-LD, meta tags, OpenGraph
- **Selectores CSS M√∫ltiples**: Fallbacks para diferentes layouts
- **APIs Complementarias**: PubMed Entrez para enriquecimiento
- **Expresiones Regulares**: Para identificadores y patrones espec√≠ficos

### Fase Transform (Transformaci√≥n)

```python
def standardize_format(self, articles: List[Article]) -> List[Dict]:
    """
    1. Normalizaci√≥n de metadatos
    2. Detecci√≥n y eliminaci√≥n de duplicados
    3. Conversi√≥n a JSON-LD con schema.org
    4. Generaci√≥n de triplas RDF
    5. Extracci√≥n de entidades para knowledge graph
    """
```

**Transformaciones Aplicadas:**
- **Normalizaci√≥n de fechas**: M√∫ltiples formatos ‚Üí ISO 8601
- **Estandarizaci√≥n de autores**: Nombre completo + afiliaci√≥n
- **Limpieza de texto**: Espacios, caracteres especiales, encoding
- **Mapping sem√°ntico**: Entidades locales ‚Üí ontolog√≠as est√°ndar

### Fase Load (Carga)

```python
# Formatos de salida generados
outputs = {
    'raw_articles': Article dataclass objects,
    'json_ld': Schema.org structured data,
    'rdf_triples': RDF triplas para graph databases,
    'graph_entities': Entidades estructuradas para KG,
    'embeddings': BioBERT semantic vectors
}
```

## üìà M√©tricas y Calidad

### Sistema de Scoring

```python
def calculate_quality_score(self, article: Article) -> float:
    """
    Score de 0.0 a 1.0 basado en:
    - Completitud de metadatos (25%)
    - Disponibilidad de texto completo (30%)  
    - Presencia de identificadores (20%)
    - Calidad de autores/afiliaciones (15%)
    - Keywords y t√©rminos MeSH (10%)
    """
```

### Criterios de Validaci√≥n

| Criterio | Peso | Validaci√≥n |
|----------|------|------------|
| **T√≠tulo** | 10% | Longitud > 10 caracteres, no vac√≠o |
| **Abstract** | 20% | Longitud > 100 caracteres |
| **Texto Completo** | 20% | Longitud > 1000 caracteres |
| **Identificadores** | 15% | PMID/PMCID/DOI v√°lidos |
| **Metadatos** | 15% | Journal, fecha, autores |
| **T√©rminos Sem√°nticos** | 10% | Keywords, MeSH terms |
| **URL V√°lida** | 10% | URL accesible, dominio confiable |

### M√©tricas de Pipeline

```python
SUCCESS_METRICS = {
    'coverage': '95%',        # Fuentes procesadas exitosamente
    'quality': '90%',         # Datos que pasan validaci√≥n
    'completeness': '85%',    # Campos requeridos completos
    'throughput': '1000/hour', # Art√≠culos procesados por hora
    'deduplication': '<2%',   # Porcentaje de duplicados
}
```

## üóÇÔ∏è Formatos de Salida

### 1. Estructura de Art√≠culo (Article Dataclass)

```python
@dataclass
class Article:
    title: str
    url: str
    pmcid: Optional[str] = None
    pmid: Optional[str] = None
    doi: Optional[str] = None
    authors: List[Dict] = None
    journal: Optional[str] = None
    publication_date: Optional[datetime] = None
    abstract: Optional[str] = None
    full_text: Optional[str] = None
    keywords: List[str] = None
    mesh_terms: List[str] = None
    organisms: List[str] = None
    genes: List[str] = None
    proteins: List[str] = None
    conditions: List[str] = None
    experiments: List[str] = None
    measurements: List[str] = None
    quality_score: float = 0.0
```

### 2. JSON-LD (Schema.org + Bio Ontologies)

```json
{
  "@context": {
    "schema": "https://schema.org/",
    "bio": "http://purl.obolibrary.org/obo/",
    "dcterms": "http://purl.org/dc/terms/"
  },
  "@id": "http://space-biology.org/article/PMC4136787",
  "@type": "schema:ScholarlyArticle",
  "schema:name": "Mice in Bion-M 1 space mission",
  "schema:author": [{
    "@type": "schema:Person",
    "schema:name": "John Smith",
    "schema:affiliation": {
      "@type": "schema:Organization",
      "schema:name": "NASA Ames Research Center"
    }
  }],
  "schema:isPartOf": {
    "@type": "schema:Periodical", 
    "schema:name": "Space Biology Journal"
  },
  "bio:mesh_terms": ["Microgravity", "Bone Density", "Space Flight"]
}
```

### 3. RDF Triples

```turtle
@prefix schema: <https://schema.org/> .
@prefix bio: <http://purl.obolibrary.org/obo/> .
@prefix space: <http://space-biology.org/ontology/> .

<http://space-biology.org/article/PMC4136787> a schema:ScholarlyArticle ;
    schema:name "Mice in Bion-M 1 space mission" ;
    schema:author <http://space-biology.org/author/john-smith> ;
    space:studiesOrganism <http://space-biology.org/organism/mouse> ;
    bio:hasMeshTerm <http://space-biology.org/mesh/microgravity> .
```

### 4. Knowledge Graph Entities

```json
{
  "graph_entities": {
    "Article": {
      "id": "http://space-biology.org/article/PMC4136787",
      "title": "Mice in Bion-M 1 space mission",
      "journal": "Space Biology Journal",
      "quality_score": 0.87
    },
    "Authors": [
      {"name": "John Smith", "affiliation": "NASA Ames"}
    ],
    "Organisms": ["mouse", "mice"],
    "Conditions": ["microgravity", "space flight"],
    "Experiments": ["gene expression", "protein analysis"],
    "Keywords": ["space biology", "microgravity", "bone density"]
  }
}
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Archivo de Configuraci√≥n (config.yaml)

```yaml
# Pipeline Configuration
pipeline:
  concurrency_limit: 10
  request_delay: 0.5
  timeout: 60
  max_retries: 3
  quality_threshold: 0.6

# Data Sources
sources:
  sb_publications:
    url: "https://raw.githubusercontent.com/jgalazka/SB_publications/main/SB_publications_PMC.csv"
    enabled: true
    priority: 1
  
  pubmed_api:
    base_url: "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
    rate_limit: "3/second"
    api_key: null  # Optional

# NLP Processing
nlp:
  scispacy_model: "en_core_sci_lg"
  biobert_model: "dmis-lab/biobert-v1.1"
  max_text_length: 5000
  entity_confidence: 0.7

# Output Configuration
output:
  formats: ["json", "json-ld", "rdf", "csv"]
  include_embeddings: true
  include_full_text: false
  timestamp_suffix: true

# Quality Control
quality:
  min_title_length: 10
  min_abstract_length: 100
  min_fulltext_length: 1000
  required_fields: ["title", "url"]
  duplicate_similarity_threshold: 85

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "space_biology_pipeline.log"
  max_size: "10MB"
  backup_count: 5
```

### Variables de Entorno

```bash
# Configuraci√≥n de APIs
export PUBMED_API_KEY="your_api_key_here"
export ENTREZ_EMAIL="your_email@domain.com"

# Configuraci√≥n de recursos
export MAX_WORKERS=10
export MEMORY_LIMIT="8GB"
export CACHE_TTL=3600

# Configuraci√≥n de salida
export OUTPUT_DIR="./output"
export LOG_LEVEL="INFO"
```

### Personalizaci√≥n del Pipeline

```python
# Custom entity extractors
class CustomSpaceBiologyExtractor:
    def extract_space_conditions(self, text: str) -> List[str]:
        # Implementar l√≥gica custom
        pass

# Custom validation rules  
class CustomQualityValidator:
    def validate_space_relevance(self, article: Article) -> bool:
        # Validar relevancia para biolog√≠a espacial
        space_keywords = ['microgravity', 'space', 'astronaut', 'ISS']
        return any(kw in article.title.lower() for kw in space_keywords)

# Integrar en pipeline
pipeline.add_custom_extractor(CustomSpaceBiologyExtractor())
pipeline.add_custom_validator(CustomQualityValidator())
```

## üß™ Testing y Validaci√≥n

### Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py
‚îÇ   ‚îú‚îÄ‚îÄ test_nlp_processing.py
‚îÇ   ‚îú‚îÄ‚îÄ test_format_conversion.py
‚îÇ   ‚îî‚îÄ‚îÄ test_quality_validation.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_full_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api_integration.py
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îú‚îÄ‚îÄ sample_articles.json
‚îÇ   ‚îî‚îÄ‚îÄ test_data.csv
‚îî‚îÄ‚îÄ conftest.py
```

### Ejecutar Tests

```bash
# Tests unitarios
pytest tests/unit/ -v

# Tests de integraci√≥n
pytest tests/integration/ -v

# Tests con cobertura
pytest --cov=space_biology_etl tests/

# Tests de performance
pytest tests/performance/ --benchmark-only

# Tests espec√≠ficos
pytest tests/unit/test_extraction.py::test_pmcid_extraction -v
```

### Ejemplos de Tests

```python
import pytest
from space_biology_etl import DataIngestionPipeline, Article

@pytest.mark.asyncio
async def test_article_extraction():
    """Test extracci√≥n b√°sica de art√≠culo"""
    pipeline = DataIngestionPipeline()
    
    sample_data = {
        'title': 'Test Article',
        'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC123456/'
    }
    
    # Mock response
    with pytest.mock.patch('aiohttp.ClientSession.get') as mock_get:
        mock_get.return_value.__aenter__.return_value.text.return_value = sample_html
        
        article = await pipeline.extract_single_paper(sample_data)
        
        assert article is not None
        assert article.title == 'Test Article'
        assert article.pmcid == 'PMC123456'

def test_quality_scoring():
    """Test sistema de scoring de calidad"""
    pipeline = DataIngestionPipeline()
    
    # Art√≠culo de alta calidad
    good_article = Article(
        title="Complete Article Title",
        abstract="This is a comprehensive abstract...",
        full_text="Full article content...",
        authors=[{"name": "John Doe"}],
        doi="10.1234/test"
    )
    
    score = pipeline.calculate_quality_score(good_article)
    assert score > 0.8
    
    # Art√≠culo de baja calidad
    bad_article = Article(title="Short", abstract="", full_text="")
    score = pipeline.calculate_quality_score(bad_article)
    assert score < 0.3
```

### Validaci√≥n de Datos

```python
# Validadores autom√°ticos
validators = [
    URLValidator(),           # URLs v√°lidas y accesibles
    DateValidator(),          # Fechas en rango razonable  
    IdentifierValidator(),    # DOI/PMID/PMCID v√°lidos
    TextQualityValidator(),   # Calidad de contenido textual
    SemanticValidator(),      # Coherencia sem√°ntica
    DuplicateValidator()      # Detecci√≥n de duplicados
]

# Ejecutar validaciones
for validator in validators:
    results = validator.validate(articles)
    logger.info(f"{validator.__class__.__name__}: {results.summary()}")
```

## üìù Logging y Monitoreo

### Configuraci√≥n de Logging

```python
import logging
from logging.handlers import RotatingFileHandler

# Configuraci√≥n avanzada de logging
def setup_logging():
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Handler para archivo
    file_handler = RotatingFileHandler(
        'space_biology_pipeline.log', 
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(formatter)
    
    # Handler para consola
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # Logger principal
    logger = logging.getLogger('space_biology_etl')
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger
```

### M√©tricas de Monitoreo

```python
class PipelineMonitor:
    def __init__(self):
        self.start_time = time.time()
        self.articles_processed = 0
        self.articles_failed = 0
        self.total_entities_extracted = 0
        
    def log_progress(self):
        elapsed = time.time() - self.start_time
        rate = self.articles_processed / elapsed if elapsed > 0 else 0
        
        logger.info(f"""
        === PIPELINE PROGRESS ===
        Elapsed Time: {elapsed:.2f}s
        Articles Processed: {self.articles_processed}
        Articles Failed: {self.articles_failed}
        Success Rate: {self.articles_processed/(self.articles_processed + self.articles_failed)*100:.1f}%
        Processing Rate: {rate:.2f} articles/second
        Entities Extracted: {self.total_entities_extracted}
        """)

# Usar en pipeline
monitor = PipelineMonitor()
for article in articles:
    try:
        processed = await process_article(article)
        monitor.articles_processed += 1
        monitor.total_entities_extracted += len(processed.organisms) + len(processed.genes)
    except Exception as e:
        monitor.articles_failed += 1
        logger.error(f"Failed processing {article.title}: {e}")
    
    if monitor.articles_processed % 10 == 0:
        monitor.log_progress()
```

### Dashboard de Monitoreo (Opcional)

```python
# Integraci√≥n con Prometheus/Grafana
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# M√©tricas Prometheus
articles_processed = Counter('articles_processed_total', 'Total articles processed')
processing_time = Histogram('article_processing_seconds', 'Time to process article')  
quality_score = Gauge('average_quality_score', 'Average quality score')
extraction_errors = Counter('extraction_errors_total', 'Total extraction errors')

# Usar en c√≥digo
@processing_time.time()
async def process_article(article):
    # L√≥gica de procesamiento
    articles_processed.inc()
    return processed_article

# Iniciar servidor de m√©tricas
start_http_server(8000)
```

## ü§ù Contribuci√≥n

### C√≥mo Contribuir

1. **Fork** el repositorio
2. **Crear** branch para feature (`git checkout -b feature/amazing-feature`)
3. **Commit** cambios (`git commit -m 'Add amazing feature'`)
4. **Push** al branch (`git push origin feature/amazing-feature`)
5. **Abrir** Pull Request

### Gu√≠as de Desarrollo

#### Estilo de C√≥digo
```bash
# Usar black para formatting
black space_biology_etl.py

# Usar flake8 para linting
flake8 space_biology_etl.py --max-line-length=88

# Usar mypy para type checking
mypy space_biology_etl.py
```

#### Estructura de Commits
```
feat: add new entity extractor for space conditions
fix: resolve duplicate detection false positives  
docs: update README with configuration examples
test: add unit tests for NLP processor
refactor: optimize async processing performance
```

#### Pre-commit Hooks
```yaml
# .pre-commit-config.yaml
repos:
-   repo: https://github.com/psf/black
    rev: 22